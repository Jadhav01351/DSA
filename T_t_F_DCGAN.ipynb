{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jadhav01351/DSA/blob/main/T_t_F_DCGAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y-fak5eHsd07"
      },
      "source": [
        "# Import dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sIIlMdsZBDw6"
      },
      "source": [
        "import os\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from PIL import Image\n",
        "from tqdm.notebook import tqdm\n",
        "from dataclasses import asdict, dataclass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-fSp7nivA-5m"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V0NWzJXZyLEG",
        "outputId": "dc6ab46b-6c81-43da-fcb4-11bdfcaf83cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.29.2-py3-none-any.whl (7.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m55.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.0)\n",
            "Collecting huggingface-hub<1.0,>=0.14.1 (from transformers)\n",
            "  Downloading huggingface_hub-0.14.1-py3-none-any.whl (224 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m27.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m124.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.14.1 tokenizers-0.13.3 transformers-4.29.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z560oZu3qeCF"
      },
      "source": [
        "from transformers import DistilBertTokenizer, DistilBertModel, BertTokenizer, BertModel"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install preprocess"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eDs_nVlMyV8v",
        "outputId": "e98b91a5-4c43-4b31-af36-59730d801bdf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting preprocess\n",
            "  Downloading preprocess-2.0.0-py3-none-any.whl (12 kB)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.10/dist-packages (from preprocess) (0.18.3)\n",
            "Installing collected packages: preprocess\n",
            "Successfully installed preprocess-2.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-UzYK2kLpj9z",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "outputId": "ad6f0a0b-d475-49a9-fdb6-85215fe82850"
      },
      "source": [
        "from preprocess import get_weighted_dataloader, extract_zip\n",
        "from text_encoder.sentence_encoder import SentenceEncoder"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-2bd2efdda3a9>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpreprocess\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_weighted_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextract_zip\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtext_encoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentence_encoder\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSentenceEncoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'get_weighted_dataloader' from 'preprocess' (/usr/local/lib/python3.10/dist-packages/preprocess.py)",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3mCVww2wK2Hs"
      },
      "source": [
        "# Extracting the Dataset\n",
        "\n",
        "You can download the dataset from Kaggle - https://www.kaggle.com/jessicali9530/celeba-dataset\n",
        "\n",
        "Or from https://mmlab.ie.cuhk.edu.hk/projects/CelebA.html"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "metadata": {
        "id": "sIJxxi0xwVXC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MBgqeVGBmjWL"
      },
      "source": [
        "# File Constants\n",
        "ZIP_PATH = \"/content/drive/MyDrive/Major Project/archive (26).zip\"\n",
        "OUTPUT_DIR = \"/content/drive/MyDrive/Major Project/extract/\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_cjD-p8oqgd0"
      },
      "source": [
        "#extract_zip(ZIP_PATH, OUTPUT_DIR)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "skxZ1E02BK_I"
      },
      "source": [
        "# Data Loading"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q5hY58Wa4_qV"
      },
      "source": [
        "## Initialize Constants"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-NCehNC-qrnN"
      },
      "source": [
        "@dataclass\n",
        "class Config:\n",
        "  epochs: int = 20\n",
        "  image_size: int = 128\n",
        "  initial_size: int = 64\n",
        "  noise_size: int = 100\n",
        "  batch_size: int = 64\n",
        "  subset_size: int = 20_000\n",
        "  num_channels: int = 3\n",
        "\n",
        "  device: 'typing.Any' = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AsLMEKAqrZz-"
      },
      "source": [
        "cfg = Config()\n",
        "cfg_dict = asdict(cfg)\n",
        "wandb.config.update(cfg_dict)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oCEA1yGyrbj1"
      },
      "source": [
        "sentence_encoder = SentenceEncoder(cfg.device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7v8ompoWyt5H"
      },
      "source": [
        "# Initialize Helpers (for images)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "umMVWg4Syx5w"
      },
      "source": [
        "def show_grid(img):\n",
        "  npimg = img.numpy()\n",
        "  plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FLdKTg1b41Ph"
      },
      "source": [
        "## Creating subset of dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qnJB9n0csQ2I"
      },
      "source": [
        "weighted_dataloader, weighted_dataiter = get_weighted_dataloader(\n",
        "    '/content/FGTD/dataset/list_attr_celeba.csv',\n",
        "    image_location='/content/drive/MyDrive/Major Project/extract/img_align_celeba/img_align_celeba',\n",
        "    text_desc_location='/content/FGTD/dataset/text_descr_celeba.csv',\n",
        "    transform=transforms.Compose([transforms.Resize((cfg.image_size, cfg.image_size)), transforms.ToTensor(), transforms.Normalize(mean=(0.5), std=(0.5))]),\n",
        "    batch_size=cfg.batch_size,\n",
        "    subset_size=cfg.subset_size\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VZxPBCrd0Uy_"
      },
      "source": [
        "## Testing Dataloader output"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JnODdQguFwYs"
      },
      "source": [
        "images, labels, wrong_images = next(weighted_dataiter)\n",
        "show_grid(torchvision.utils.make_grid(images, normalize=True))\n",
        "print(labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ED9Dc-KL52nJ"
      },
      "source": [
        "# Initialize Weights"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hSCtKgvb52Yj"
      },
      "source": [
        "def initialize_weights(model):\n",
        "    className = model.__class__.__name__\n",
        "    if className.find('Conv') != -1:\n",
        "        nn.init.normal_(model.weight.data, 0.0, 0.02)\n",
        "    elif className.find('BatchNorm') != -1:\n",
        "        nn.init.normal_(model.weight.data, 1.0, 0.02)\n",
        "        nn.init.constant_(model.bias.data, 0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yXOSQFJ8ZXKC"
      },
      "source": [
        "# Generator Network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7enKRrb5gL61"
      },
      "source": [
        "## Create Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CSZZKtPGDAgN"
      },
      "source": [
        "class Generator(nn.Module):\n",
        "    '''\n",
        "    The Generator Network\n",
        "    '''\n",
        "\n",
        "    def __init__(self, noise_size, feature_size, num_channels, embedding_size, reduced_dim_size):\n",
        "        super(Generator, self).__init__()\n",
        "        self.reduced_dim_size = reduced_dim_size\n",
        "\n",
        "        #element-wise function\n",
        "        self.projection = nn.Sequential(  #A sequential container. Modules will be added to it in the order they are passed in the constructor\n",
        "            nn.Linear(in_features = embedding_size, out_features = reduced_dim_size), #Applies a linear transformation to the incoming data: y=xA T +b\n",
        "            nn.BatchNorm1d(num_features = reduced_dim_size), #helps to stabilize the network during training. Batch Norm is just another network layer that gets inserted between a hidden layer and the next hidden layer. Its job is to take the outputs from the first hidden layer and normalize them before passing them on as the input of the next hidden layer.\n",
        "            nn.LeakyReLU(negative_slope = 0.2, inplace = True) #Leaky ReLU activation functionf(x)=max(0.01*x , x). This function returns x if it receives any positive input, but for any negative value of x, it returns a really small value which is 0.01 times x. Thus it gives an output for negative values as well.\n",
        "        )\n",
        "\n",
        "        self.layer = nn.Sequential(\n",
        "            nn.ConvTranspose2d(noise_size + reduced_dim_size, feature_size * 8, 4, 1, 0, bias = False),\n",
        "            nn.BatchNorm2d(feature_size * 8),\n",
        "            nn.LeakyReLU(negative_slope = 0.2, inplace = True),\n",
        "\n",
        "            # state size (ngf*4) x 4 x 4\n",
        "            nn.ConvTranspose2d(feature_size * 8, feature_size * 4, 4, 2, 1, bias = False),\n",
        "            nn.BatchNorm2d(feature_size * 4),\n",
        "            nn.ReLU(True),\n",
        "\n",
        "            # state size. (ngf*4) x 8 x 8\n",
        "            nn.ConvTranspose2d(feature_size * 4, feature_size * 2, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(feature_size * 2),\n",
        "            nn.ReLU(True),\n",
        "\n",
        "            # state size. (ngf*2) x 16 x 16\n",
        "            nn.ConvTranspose2d(feature_size * 2, feature_size, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(feature_size),\n",
        "            nn.ReLU(True),\n",
        "\n",
        "            # state size. (ngf*2) x 32 x 32\n",
        "            nn.ConvTranspose2d(feature_size, feature_size, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(feature_size),\n",
        "            nn.ReLU(True),\n",
        "\n",
        "            # state size. (ngf) x 64 x 64\n",
        "            nn.ConvTranspose2d(feature_size, num_channels, 4, 2, 1, bias=False),\n",
        "            nn.Tanh() #These images are normalized between [-1, 1] rather than [0,1] , thus Tanh over sigmoid\n",
        "\n",
        "        )\n",
        "        self.optimizer = torch.optim.Adam(self.parameters(), lr = 0.0002, betas = (0.5, 0.5))\n",
        "\n",
        "    def forward(self, noise, text_embeddings):\n",
        "        encoded_text = self.projection(text_embeddings)\n",
        "        concat_input = torch.cat([noise, encoded_text], dim = 1).unsqueeze(2).unsqueeze(2)\n",
        "        output = self.layer(concat_input)\n",
        "        return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n19svrGxd1QX"
      },
      "source": [
        "generator = Generator(cfg.noise_size, cfg.image_size, cfg.num_channels, 768, 256)\n",
        "generator.apply(initialize_weights)\n",
        "generator.cuda()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HmYIdH8RgGiW"
      },
      "source": [
        "## Test output shape of Generator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z1NbA9vXd63h"
      },
      "source": [
        "with torch.no_grad():\n",
        "  generator.eval()\n",
        "  noise = torch.randn(size=(cfg.batch_size, cfg.noise_size)).cuda()\n",
        "  text_embeddings = sentence_encoder.convert_text_to_embeddings(labels)\n",
        "  output = generator(noise, text_embeddings).detach()\n",
        "  print(output.shape)\n",
        "show_grid(torchvision.utils.make_grid(output.cpu(), normalize=True))\n",
        "generator.train()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zn6qKiOR9u2g"
      },
      "source": [
        "# Discriminator Network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QrSCZY6MjTxq"
      },
      "source": [
        "## Create Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3jePSBQZIU3y"
      },
      "source": [
        "class Discriminator(nn.Module):\n",
        "    '''\n",
        "    The Discriminator Network\n",
        "    '''\n",
        "\n",
        "    def __init__(self, num_channels, feature_size, embedding_size, reduced_dim_size):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.reduced_dim_size = reduced_dim_size\n",
        "\n",
        "        ## Image Encoder\n",
        "        self.netD_1 = nn.Sequential(\n",
        "            # input nc x 128 x 128\n",
        "            nn.Conv2d(num_channels, feature_size, 4, 2, 1, bias = False),\n",
        "            nn.LeakyReLU(0.2, inplace = True),\n",
        "\n",
        "            # input nc x 64 x 64\n",
        "            nn.Conv2d(feature_size, feature_size, 4, 2, 1, bias = False),\n",
        "            nn.LeakyReLU(0.2, inplace = True),\n",
        "\n",
        "            # state size ndf x 32 x 32\n",
        "            nn.Conv2d(feature_size, feature_size * 2, 4, 2, 1, bias = False),\n",
        "            nn.BatchNorm2d(feature_size * 2),\n",
        "            nn.LeakyReLU(0.2, inplace = True),\n",
        "\n",
        "            # state size. (ndf*2) x 16 x 16\n",
        "            nn.Conv2d(feature_size * 2, feature_size * 4, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(feature_size * 4),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "\n",
        "            # state size. (ndf*4) x 8 x 8\n",
        "            nn.Conv2d(feature_size * 4, feature_size * 8, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(feature_size * 8),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "\n",
        "        )\n",
        "\n",
        "        ## Text Encoder\n",
        "        self.projector = nn.Sequential(\n",
        "            nn.Linear(in_features=embedding_size, out_features=reduced_dim_size),\n",
        "            nn.BatchNorm1d(num_features=reduced_dim_size),\n",
        "            nn.LeakyReLU(negative_slope=0.2, inplace=True)\n",
        "        )\n",
        "\n",
        "        ## Text + Image Concat Layer\n",
        "        self.netD_2 = nn.Sequential(\n",
        "            # state size. (ndf*8) x 4 x 4\n",
        "            nn.Conv2d(feature_size * 8 + reduced_dim_size, 1, 4, 1, 0, bias=False),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "        self.optimizer = torch.optim.Adam(self.parameters(), lr = 0.0001, betas = (0.5, 0.5))\n",
        "\n",
        "    def forward(self, input_img, text_embeddings):\n",
        "        x_intermediate = self.netD_1(input_img)\n",
        "\n",
        "        projected_embed = self.projector(text_embeddings)\n",
        "\n",
        "        replicated_embed = projected_embed.repeat(4, 4, 1, 1).permute(2,  3, 0, 1)\n",
        "        hidden_concat = torch.cat([x_intermediate, replicated_embed], 1)\n",
        "\n",
        "        x = self.netD_2(hidden_concat)\n",
        "\n",
        "        return x.view(-1, 1), x_intermediate"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kJGBac5r_T2f"
      },
      "source": [
        "discriminator = Discriminator(cfg.num_channels, cfg.image_size, 768, 256)\n",
        "discriminator.apply(initialize_weights)\n",
        "discriminator.cuda()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vcl-pGt6jZbC"
      },
      "source": [
        "## Test output shape of Discriminator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hdsXthvtjf__"
      },
      "source": [
        "with torch.no_grad():\n",
        "  discriminator.eval()\n",
        "  validity, _ = discriminator(output, text_embeddings)\n",
        "  print(validity.shape)\n",
        "discriminator.train()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FnWTOSO0oQr9"
      },
      "source": [
        "# Specifying Loss Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ul76_v3uCsxD"
      },
      "source": [
        "criterion = nn.BCELoss().cuda()\n",
        "l2_loss = nn.MSELoss().cuda()\n",
        "l1_loss = nn.L1Loss().cuda()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tn8NiOpBgqmy"
      },
      "source": [
        "# Plotting output after each epoch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2oXr90uDtRNX"
      },
      "source": [
        "This is just to generate a batch of text embeddings to be used to plot"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DGc4GWEq_FbN"
      },
      "source": [
        "plt_images, plt_labels, _ = next(weighted_dataiter)\n",
        "plt_o_text_embeddings = sentence_encoder.convert_text_to_embeddings(plt_labels)\n",
        "plt_o_text_embeddings.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WuNc4_f0RRy6"
      },
      "source": [
        "show_grid(torchvision.utils.make_grid(plt_images.cpu(), normalize=True))\n",
        "print(plt_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wmt-n-onWqIs"
      },
      "source": [
        "fixed_noise = torch.randn(size=(len(plt_labels), cfg.noise_size)).cuda()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JFbwj0-3oHyW"
      },
      "source": [
        "def plot_output(epoch):\n",
        "  plt.clf()\n",
        "  with torch.no_grad():\n",
        "\n",
        "    generator.eval()\n",
        "    test_images = generator(fixed_noise, plt_o_text_embeddings)\n",
        "    generator.train()\n",
        "\n",
        "    grid = torchvision.utils.make_grid(test_images.cpu(), normalize=True)\n",
        "    show_grid(grid)\n",
        "\n",
        "  wandb.log({'output': wandb.Image(grid, caption=f'Output for epoch: {epoch}')}, step=epoch)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BQPRzQopsLxy"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4v_TRQ09sLLr"
      },
      "source": [
        "pbar = tqdm()\n",
        "\n",
        "wandb.watch(generator)\n",
        "wandb.watch(discriminator)\n",
        "\n",
        "for epoch in range(cfg.epochs):\n",
        "    print(f'Epoch:  {epoch + 1} / {cfg.epochs}')\n",
        "    pbar.reset(total = len(weighted_dataloader))\n",
        "\n",
        "    discriminator_loss = []\n",
        "    generator_loss = []\n",
        "\n",
        "    for i, (real_images, real_text, wrong_images) in enumerate(weighted_dataloader):\n",
        "        current_batch_size = real_images.size()[0]\n",
        "\n",
        "        #converting to cuda\n",
        "        real_images = real_images.cuda()\n",
        "        text_embeddings = sentence_encoder.convert_text_to_embeddings(real_text)\n",
        "        wrong_images = wrong_images.cuda()\n",
        "\n",
        "        real_validity = torch.ones(current_batch_size, 1).cuda()\n",
        "        fake_validity = torch.zeros(current_batch_size, 1).cuda()\n",
        "\n",
        "        temp_tensor = (torch.ones(current_batch_size, 1).cuda()) * (-0.1)\n",
        "        smoothed_real_labels = torch.add(real_validity, temp_tensor)\n",
        "\n",
        "        ## Training the discriminator\n",
        "        discriminator.zero_grad()\n",
        "        output, activation_real = discriminator(real_images, text_embeddings)\n",
        "        real_loss = criterion(output, smoothed_real_labels)\n",
        "        real_score = output\n",
        "\n",
        "        output, _ = discriminator(wrong_images, text_embeddings)\n",
        "        wrong_loss = criterion(output, fake_validity)\n",
        "\n",
        "        input_noise = torch.randn(size=(current_batch_size, cfg.noise_size)).cuda()\n",
        "        fake_images = generator(input_noise, text_embeddings)\n",
        "        output, _ = discriminator(fake_images, text_embeddings)\n",
        "        fake_loss = criterion(output, fake_validity)\n",
        "\n",
        "        d_loss = real_loss + wrong_loss + fake_loss\n",
        "\n",
        "        d_loss.backward()\n",
        "        discriminator.optimizer.step()\n",
        "        discriminator_loss.append(d_loss)\n",
        "\n",
        "        ## Training generator\n",
        "        generator.zero_grad()\n",
        "        input_noise = torch.randn(size=(current_batch_size, cfg.noise_size)).cuda()\n",
        "        fake_images = generator(input_noise, text_embeddings)\n",
        "        output, activation_fake = discriminator(fake_images, text_embeddings)\n",
        "        _, activation_real = discriminator(real_images, text_embeddings)\n",
        "\n",
        "        activation_fake = torch.mean(activation_fake, 0)    #try with median and check if it converges\n",
        "        activation_real = torch.mean(activation_real, 0)    #try with median and check if it converges\n",
        "\n",
        "        g_loss = criterion(output, real_validity) + 100 * l2_loss(activation_fake, activation_real.detach()) + 50 * l1_loss(fake_images, real_images)\n",
        "\n",
        "        g_loss.backward()\n",
        "        generator.optimizer.step()\n",
        "        generator_loss.append(g_loss)\n",
        "\n",
        "        pbar.update()\n",
        "\n",
        "    print('Discriminator Loss: {:.3f}, Generator Loss: {:.3f}'.format(\n",
        "          torch.mean(torch.FloatTensor(discriminator_loss)),\n",
        "          torch.mean(torch.FloatTensor(generator_loss))\n",
        "\n",
        "  ))\n",
        "\n",
        "    wandb.log({\"generator loss\" : torch.mean(torch.FloatTensor(generator_loss)), \"discriminator loss\" : torch.mean(torch.FloatTensor(discriminator_loss))}, step=epoch+1)\n",
        "\n",
        "    if (epoch + 1) % 1 == 0:\n",
        "      plot_output(epoch + 1)\n",
        "\n",
        "pbar.refresh()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_noise = torch.randn(size=(1, cfg.noise_size)).cuda()\n",
        "test_embeddings = sentence_encoder.convert_text_to_embeddings(['The man is chubby, has a double chin and pretty high cheekbones. He grows a sideburns. His hair is gray and receding. He has big lips and a big nose. The man looks young.'])"
      ],
      "metadata": {
        "id": "gwblud2pxqHS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generator.eval()\n",
        "test_image = generator(test_noise, test_embeddings).detach().cpu()\n",
        "show_grid(torchvision.utils.make_grid(test_image, normalize=True, nrow=1))"
      ],
      "metadata": {
        "id": "NtvXxipMxtvA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_noise = torch.randn(size=(1, cfg.noise_size)).cuda()\n",
        "test_embeddings = sentence_encoder.convert_text_to_embeddings(['The female has pretty high cheekbones and an oval face. Her hair is black. She has a slightly open mouth and a pointy nose. The female is smiling, looks attractive and has heavy makeup. She is wearing earrings and lipstick.'])"
      ],
      "metadata": {
        "id": "sUaKXDZixuL5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FOuOWRyKxzNG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_image = generator(test_noise, test_embeddings).detach().cpu()\n",
        "show_grid(torchvision.utils.make_grid(test_image, normalize=True, nrow=1))"
      ],
      "metadata": {
        "id": "dZuMCAvKxzEl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}